{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECE-505 Computer Project 1\n",
    "### Due Date Nov-6 2018\n",
    "\n",
    "Step 1: Importing necessary libraries for all the necessary modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART1-Algorithmic Implementation\n",
    "#### 1.Steepest Gradient Descent Method\n",
    "Details of problem statement for implementing Steepest Gradient Descent Method is available in the file \"Computer Project I description.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unconstrained_optimizer:\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    \n",
    "    #function is the mathematical function that needs to be optimized\n",
    "    #gradient is the gadient of the mathematical function.\n",
    "    #line_search is method of finding alpha\n",
    "    def Steepest_Gradient_Descent_Method(self,function,gradient,line_search,\n",
    "                                         linear_first_derivative,condition_check,\n",
    "                                         epsilon,Iter_max,seed_x,linear_second_derivative=None,condition_param=0.5,seed_alpha=0.5,in_param=None,gamma_param=None):\n",
    "        #List to store values to be printed at each iteration\n",
    "        grad_value=[]\n",
    "        x_value=[]\n",
    "        alpha_value=[]\n",
    "        next_x=np.array(seed_x)\n",
    "        current_alpha_value=seed_alpha\n",
    "        for i in range(Iter_max):\n",
    "            #print(\"Iteration:\")\n",
    "            #print(i)\n",
    "            current_x=next_x\n",
    "            current_grad_value=gradient(current_x,in_param,gamma_param)\n",
    "            x_value.append(current_x)\n",
    "            grad_value.append(current_grad_value)\n",
    "            f_of_x=function(current_x,in_param,gamma_param)\n",
    "            criteria=np.linalg.norm(current_grad_value,2)/(1+np.absolute(f_of_x))\n",
    "            \n",
    "            if criteria <= epsilon:\n",
    "                print(\"Criteria of epsilon met\")\n",
    "                return[x_value,grad_value,alpha_value,criteria]\n",
    "            if i==25 and condition_param<1e-3:\n",
    "                condition_param=0.8\n",
    "                alpha=0.9\n",
    "            elif i==50 and condition_param<1e-3:\n",
    "                condition_param=0.8\n",
    "                alpha=0.9\n",
    "            elif i==100 and condition_param<1e-2:\n",
    "                condition_param=0.8\n",
    "                alpha=0.9\n",
    "            elif i==200 and condition_param<1e-1:\n",
    "                condition_param=0.8\n",
    "                alpha=0.9\n",
    "            elif i==300:\n",
    "                alpha=1.5\n",
    "            elif i==500 :\n",
    "                condition_param=0.8\n",
    "                alpha=1.5\n",
    "            elif i==700:\n",
    "                alpha=1.5\n",
    "            \n",
    "            p=-current_grad_value\n",
    "            current_alpha_value=line_search(current_x,p,current_alpha_value,function,current_grad_value, \n",
    "                                            linear_first_derivative,linear_second_derivative,condition_check,\n",
    "                                            (condition_param),in_param,gamma_param)\n",
    "            alpha_value.append(current_alpha_value)\n",
    "            next_x=current_x+(current_alpha_value*p)\n",
    "        \n",
    "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
    "        current_x=next_x\n",
    "        current_grad_value=gradient(current_x,in_param,gamma_param)\n",
    "        x_value.append(current_x)\n",
    "        grad_value.append(current_grad_value)\n",
    "        criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))        \n",
    "        return[x_value,grad_value,alpha_value,criteria]\n",
    "    \n",
    "    \n",
    "    def Newtons_Method(self,function,gradient_function,hessian_function,epsilon,Iter_max,seed_x,in_param=None,gamma_param=None):\n",
    "        x_value=[]\n",
    "        grad_value=[]\n",
    "        direction_value=[]\n",
    "        next_x=np.array(seed_x)\n",
    "        for i in range(Iter_max):\n",
    "            #print(\"Iteration :\")\n",
    "            #print(i)\n",
    "            current_x_value=next_x\n",
    "            current_grad_value=gradient_function(current_x_value,in_param,gamma_param)\n",
    "            f_of_x=function(current_x_value,in_param,gamma_param)\n",
    "            criteria=np.linalg.norm(current_grad_value,2)/(1+np.absolute(f_of_x))\n",
    "            x_value.append(current_x_value)\n",
    "            grad_value.append(current_grad_value)\n",
    "            if criteria <= epsilon:\n",
    "                print(\"Criteria of epsilon met\")\n",
    "                return[x_value,grad_value,direction_value,criteria]\n",
    "            current_hessian_value=hessian_function(current_x_value)\n",
    "            if np.linalg.det(current_hessian_value)<1e-5:\n",
    "                print(\"Warning: Hessian Matrix is Singluar for the current estimate of X!!!!!\")\n",
    "            \n",
    "            current_direction_value=np.linalg.lstsq(current_hessian_value,current_grad_value)[0]\n",
    "            direction_value.append(current_direction_value)\n",
    "            current_hessian_inv=np.linalg.pinv(current_hessian_value)\n",
    "            next_x=current_x_value-current_hessian_inv.dot(current_direction_value)\n",
    "            \n",
    "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
    "        current_x_value=next_x\n",
    "        current_grad_value=gradient_function(current_x_value,in_param,gamma_param)\n",
    "        criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))\n",
    "        x_value.append(current_x_value)\n",
    "        grad_value.append(current_grad_value)\n",
    "        return[x_value,grad_value,direction_value,criteria]\n",
    "    \n",
    "    def BFGS_Quasi_Newton_Method(self,function,gradient_function,line_search,\n",
    "                                 linear_first_derivative,condition_check,\n",
    "                                 epsilon,Iter_max,seed_Hessian,seed_x,\n",
    "                                 linear_second_derivative=None,condition_param=0.5,seed_alpha=0.5,in_param=None,gamma_param=None):\n",
    "        x_value=[]\n",
    "        grad_value=[]\n",
    "        direction_value=[]\n",
    "        next_x=np.matrix(seed_x).T\n",
    "        next_B=np.matrix(seed_Hessian)\n",
    "        next_gradient=np.matrix(gradient_function(next_x,in_param,gamma_param)).T\n",
    "        alpha=seed_alpha\n",
    "        for i in range(Iter_max):\n",
    "            current_x_value=next_x\n",
    "            current_B=next_B\n",
    "            current_grad_value=next_gradient\n",
    "            x_value.append(current_x_value)\n",
    "            grad_value.append(current_grad_value)\n",
    "            f_of_x=function(current_x_value,in_param,gamma_param)\n",
    "            criteria=np.linalg.norm(current_grad_value,2)/(1+np.absolute(f_of_x))\n",
    "            if criteria <= epsilon:\n",
    "                print(\"Criteria of epsilon met\")\n",
    "                return[x_value,grad_value,direction_value,criteria]\n",
    "            if np.linalg.det(current_B)<1e-5:\n",
    "                print(\"Warning:B matrix is Singluar for the current estimate of X!!!!!\")\n",
    "            \n",
    "            current_direction_value=np.linalg.lstsq(np.squeeze(np.asarray(current_B)),np.squeeze(np.asarray(current_grad_value)))[0]\n",
    "            current_direction_value=np.matrix(current_direction_value).T\n",
    "            direction_value.append(current_direction_value)\n",
    "            alpha=line_search(current_x_value,current_direction_value,alpha,function,current_grad_value, \n",
    "                                            linear_first_derivative,linear_second_derivative,condition_check,\n",
    "                                            condition_param,in_param,gamma_param)\n",
    "            next_x=current_x_value+(alpha*current_direction_value)\n",
    "            next_gradient=np.matrix(gradient_function(next_x,in_param,gamma_param)).T\n",
    "            s_k=next_x-current_x_value\n",
    "            y_k=next_gradient-current_grad_value\n",
    "            next_B=current_B-((((current_B*s_k)*(current_B*s_k).T)/(s_k.T*current_B*s_k)))+((y_k*y_k.T)/(y_k.T*s_k))\n",
    "            \n",
    "            \n",
    "        \n",
    "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
    "        current_x_value=next_x\n",
    "        current_grad_value=gradient_function(current_x_value,in_param,gamma_param)\n",
    "        criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))\n",
    "        x_value.append(current_x_value)\n",
    "        grad_value.append(current_grad_value)\n",
    "        return[x_value,grad_value,direction_value,criteria]\n",
    "     \n",
    "\n",
    "    def Conjugate_Gradient_Descent_Method(self,function,gradient_function,hessian_function,\n",
    "                                          epsilon,Iter_max,seed_x,in_param=None,gamma_param=None):\n",
    "        x_value=[]\n",
    "        grad_value=[]\n",
    "        direction_value=[]\n",
    "        next_x=np.matrix(seed_x).T\n",
    "        next_gradient=np.matrix(gradient_function(next_x,in_param,gamma_param)).T\n",
    "        next_direction=-1*next_gradient\n",
    "        for i in range(Iter_max):\n",
    "            current_x=next_x\n",
    "            current_gradient=next_gradient\n",
    "            current_direction=next_direction\n",
    "            x_value.append(current_x)\n",
    "            grad_value.append(current_gradient)\n",
    "            direction_value.append(current_direction)\n",
    "            f_of_x=function(current_x,in_param,gamma_param)\n",
    "            criteria=np.linalg.norm(current_gradient,2)/(1+np.absolute(f_of_x))\n",
    "            \n",
    "            if criteria <= epsilon:\n",
    "                print(\"Criteria of epsilon met\")\n",
    "                return[x_value,grad_value,direction_value,criteria]\n",
    "           \n",
    "            alpha=-1*np.asscalar((current_direction.T*current_gradient)/(current_direction.T*np.matrix(hessian_function(current_x))*current_direction))\n",
    "            next_x=current_x+alpha*current_direction\n",
    "            next_gradient=np.matrix(gradient_function(next_x,in_param,gamma_param)).T\n",
    "            beta=np.asscalar((current_direction.T*next_gradient)/(current_direction.T*np.matrix(hessian_function(current_x))*current_direction))\n",
    "            next_direction=-1*next_gradient+(beta*current_direction)\n",
    "        \n",
    "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
    "        current_x=next_x\n",
    "        current_gradient=next_gradient\n",
    "        current_direction=next_direction\n",
    "        criteria=np.linalg.norm(current_gradient,2)/(1+np.linalg.norm(current_gradient,1))\n",
    "        x_value.append(current_x)\n",
    "        grad_value.append(current_gradient)\n",
    "        direction_value.append(current_direction)\n",
    "        return[x_value,grad_value,direction_value,criteria]\n",
    "    \n",
    "\n",
    "def Newton_line_search(x,p,alpha,function,grad_val,linear_first_derivative,linear_second_derivative,condition_check,condition_param,in_param,gamma_param):\n",
    "    next_alpha=alpha\n",
    "    condition=condition_check(condition_param,function,grad_val,x,p,next_alpha,in_param,gamma_param)\n",
    "    while(condition==False):\n",
    "        current_alpha=next_alpha\n",
    "        first_deri=linear_first_derivative(current_alpha,x,p,in_param,gamma_param)\n",
    "        second_deri=linear_second_derivative(current_alpha,x,p,in_param,gamma_param)\n",
    "        next_alpha=current_alpha-(first_deri/second_deri)\n",
    "        condition=condition_check(condition_param,function,grad_val,x,p,next_alpha,in_param,gamma_param)\n",
    "        \n",
    "    return next_alpha\n",
    "\n",
    "#def secant_line_search(x,p,alpha,function,linear_first_derivative,linear_second_derivative,condition_check,condition_param):\n",
    "#    condition=False\n",
    "#    next_alpha=alpha\n",
    "#    while(condition==False):\n",
    "#        current_alpha=next_alpha\n",
    "#        first_deri=linear_first_derivative(current_alpha,x,p)\n",
    "#        second_deri=linear_second_derivative(current_alpha,x,p)\n",
    "#        next_alpha=current_alpha-(first_deri/second_deri)\n",
    "#        condition=condition_check(condition_param,function,x,p,next_alpha)\n",
    "#        \n",
    "#    return next_alpha   \n",
    "\n",
    "#def Wolfe_Condition(condition_param,function,x,p,next_alpha):\n",
    "#    pass\n",
    "\n",
    "def Armijo_Condition(condition_param,function,grad_val,x,p,alpha,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    p=np.squeeze(np.asarray(p))\n",
    "    grad_val=np.squeeze(np.asarray(grad_val))\n",
    "    LHS=function(x+alpha*p,in_param,gamma_param)\n",
    "    RHS=function(x,in_param,gamma_param)+(condition_param*alpha*np.dot(p,grad_val))\n",
    "    if LHS<=RHS:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criteria of epsilon met\n",
      "Values of crit :\n",
      "0.0\n",
      "last value of x:\n",
      "[0. 0. 0.]\n",
      "Criteria of epsilon met\n",
      "Values of crit :\n",
      "6.607249479484458e-06\n",
      "last value of x:\n",
      "[1.90734863e-06 1.90734863e-06 1.90734863e-06]\n",
      "Criteria of epsilon met\n",
      "Values of crit :\n",
      "0.0\n",
      "last value of x:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Criteria of epsilon met\n",
      "Values of crit :\n",
      "0.0\n",
      "last value of x:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushal/anaconda3/envs/ece505p1/lib/python3.7/site-packages/ipykernel_launcher.py:88: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/home/kushal/anaconda3/envs/ece505p1/lib/python3.7/site-packages/ipykernel_launcher.py:126: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    }
   ],
   "source": [
    "def function_1_x(x,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=3:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 3\")\n",
    "    else:\n",
    "        return x[0]**2+x[1]**2+x[2]**2\n",
    "\n",
    "def grad_function_1_x(x,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=3:\n",
    "        print(len(x))\n",
    "        sys.exit(\"fuction1 can handle only vector of size 3\")\n",
    "    else:\n",
    "        return np.array([2*x[0],2*x[1],2*x[2]])\n",
    "    \n",
    "def hessian_function_1_x(x):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=3:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 3\")\n",
    "    else:\n",
    "        return np.array([[2,0,0],[0,2,0],[0,0,2]])\n",
    "\n",
    "def linear_first_derivative_function_1_alpha(alpha,x,p,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    p=np.squeeze(np.asarray(p))\n",
    "    return 2*np.dot(x+alpha*p,p) \n",
    "\n",
    "def linear_second_derivative_function_1_alpha(alpha,x,p,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    p=np.squeeze(np.asarray(p))\n",
    "    return 2*np.dot(p,p)\n",
    "\n",
    "optimize=unconstrained_optimizer()\n",
    "[xval,graval,alval,crit]=optimize.Steepest_Gradient_Descent_Method(function_1_x,grad_function_1_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_1_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([1,1,1]),linear_second_derivative_function_1_alpha,\n",
    "                                                                          0.5,0.5)\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crit)\n",
    "print('last value of x:')\n",
    "print(xval[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.Newtons_Method(function_1_x,grad_function_1_x,hessian_function_1_x,1e-5,1000,np.array([1,1,1]))\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.BFGS_Quasi_Newton_Method(function_1_x,grad_function_1_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_1_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([[1,0,0],[0,1,0],[0,0,1]]),np.array([1,1,1]),linear_second_derivative_function_1_alpha,\n",
    "                                                                          0.5,0.5)\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.Conjugate_Gradient_Descent_Method(function_1_x,grad_function_1_x,hessian_function_1_x,\n",
    "                                                                          1e-5,1000,np.array([1,1,1]))\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criteria of epsilon met\n",
      "Values of crit :\n",
      "8.601242601813983e-06\n",
      "last value of x:\n",
      "[0.99998084 0.99998816]\n",
      "Criteria of epsilon met\n",
      "Values of crit :\n",
      "9.108862087525039e-06\n",
      "last value of x:\n",
      "[1.00000183 0.99999704]\n",
      "Criteria of epsilon met\n",
      "Values of crit :\n",
      "8.93918681104489e-06\n",
      "last value of x:\n",
      "[[0.99998212]\n",
      " [0.99999106]]\n",
      "Criteria of epsilon met\n",
      "Values of crit :\n",
      "7.62939453169409e-06\n",
      "last value of x:\n",
      "[[0.99998474]\n",
      " [0.99999237]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushal/anaconda3/envs/ece505p1/lib/python3.7/site-packages/ipykernel_launcher.py:88: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/home/kushal/anaconda3/envs/ece505p1/lib/python3.7/site-packages/ipykernel_launcher.py:126: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    }
   ],
   "source": [
    "def function_2_x(x,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return x[0]**2+2*x[1]**2-2*x[0]*x[1]-2*x[1]\n",
    "\n",
    "def grad_function_2_x(x,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return np.array([2*x[0]-2*x[1],4*x[1]-2*x[0]-2])\n",
    "\n",
    "def hessian_function_2_x(x):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    return np.array([[2,-2],[-2,4]])\n",
    "\n",
    "def linear_first_derivative_function_2_alpha(alpha,x,p,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    p=np.squeeze(np.asarray(p))\n",
    "    nx=x+alpha*p\n",
    "    return 2*nx[0]*p[0]+4*nx[1]*p[1]-2*p[0]*nx[1]-2*p[1]*nx[0]-2*p[1]\n",
    "\n",
    "def linear_second_derivative_function_2_alpha(alpha,x,p,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    p=np.squeeze(np.asarray(p))\n",
    "    return 2*p[0]**2+4*p[1]**2-4*p[1]*p[0]\n",
    "\n",
    "optimize=unconstrained_optimizer()\n",
    "[xval,graval,alval,crit]=optimize.Steepest_Gradient_Descent_Method(function_2_x,grad_function_2_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_2_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([0,0]),linear_second_derivative_function_2_alpha,\n",
    "                                                                          0.01,1)\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crit)\n",
    "print('last value of x:')\n",
    "print(xval[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.Newtons_Method(function_2_x,grad_function_2_x,hessian_function_2_x,1e-5,1000,np.array([0,0]))\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.BFGS_Quasi_Newton_Method(function_2_x,grad_function_2_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_2_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([[1,0],[0,1]]),np.array([0,0]),linear_second_derivative_function_2_alpha,\n",
    "                                                                          0.5,0.5)\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.Conjugate_Gradient_Descent_Method(function_2_x,grad_function_2_x,hessian_function_2_x,\n",
    "                                                                          1e-5,1000,np.array([0,0]))\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_3_x(x,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return (100*(x[1]-x[0]**2)**2)+((1-x[0])**2)\n",
    "\n",
    "def grad_function_3_x(x,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return np.array([200*(x[1]-x[0]**2)*(-2*x[0])+(-2*(1-x[0])),200*(x[1]-x[0]**2)])\n",
    "\n",
    "def hessian_function_3_x(x):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return np.array([[(1200*x[0]**2)-(400*x[1])+2,-400*x[0]],[-400*x[0],200]])\n",
    "\n",
    "def linear_first_derivative_function_3_alpha(alpha,x,p,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    p=np.squeeze(np.asarray(p))\n",
    "    nx=x+alpha*p\n",
    "    return 200*(nx[1]-nx[0]**2)*(p[1]-2*nx[0]*p[0])-2*(1-nx[0])*p[0]\n",
    "\n",
    "def linear_second_derivative_function_3_alpha(alpha,x,p,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    p=np.squeeze(np.asarray(p))\n",
    "    nx=x+alpha*p\n",
    "    return 200*(p[1]-2*nx[0]*p[0])*(-2*p[0]**2)+2*p[0]**2\n",
    "\n",
    "optimize=unconstrained_optimizer()\n",
    "[xval,graval,alval,crit]=optimize.Steepest_Gradient_Descent_Method(function_3_x,grad_function_3_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_3_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([-1.2,1]),linear_second_derivative_function_3_alpha,\n",
    "                                                                          1e-4,1e-3)\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crit)\n",
    "print('last value of x:')\n",
    "print(xval[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.Newtons_Method(function_3_x,grad_function_3_x,hessian_function_3_x,1e-5,1000,np.array([-1.2,1]))\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.BFGS_Quasi_Newton_Method(function_3_x,grad_function_3_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_3_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([[1,0],[0,1]]),np.array([-1.2,1]),linear_second_derivative_function_3_alpha,\n",
    "                                                                          0.5,0.9)\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])\n",
    "\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.Conjugate_Gradient_Descent_Method(function_3_x,grad_function_3_x,hessian_function_3_x,\n",
    "                                                                          1e-5,1000,np.array([-1.2,1]))\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def function_4_x(x,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return (x[0]+x[1])**4 + x[1]**2\n",
    "\n",
    "def grad_function_4_x(x,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return np.array([4*(x[0]+x[1])**3,4*(x[0]+x[1])**3+ 2*x[1]])\n",
    "    \n",
    "def hessian_function_4_x(x):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return np.array([[12*(x[0]+x[1])**2,12*(x[0]+x[1])**2],[12*(x[0]+x[1])**2,12*(x[0]+x[1])**2+2]])\n",
    "\n",
    "def linear_first_derivative_function_4_alpha(alpha,x,p,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    p=np.squeeze(np.asarray(p))\n",
    "    nx=x+alpha*p\n",
    "    return (4*((nx[0]+nx[1])**3)*(p[0]+p[1]))+2*nx[1]*p[1]\n",
    "\n",
    "def linear_second_derivative_function_4_alpha(alpha,x,p,in_param,gamma_param):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    p=np.squeeze(np.asarray(p))\n",
    "    nx=x+alpha*p\n",
    "    return (12*((nx[0]+nx[1])**2)*(p[0]+p[1])**2)+2*p[1]**2\n",
    "\n",
    "optimize=unconstrained_optimizer()\n",
    "[xval,graval,alval,crit]=optimize.Steepest_Gradient_Descent_Method(function_4_x,grad_function_4_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_4_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([2,-2]),linear_second_derivative_function_4_alpha,\n",
    "                                                                          1e-6,0.5)\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crit)\n",
    "print('last value of x:')\n",
    "print(xval[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.Newtons_Method(grad_function_4_x,hessian_function_4_x,1e-5,1000,np.array([2,-2]))\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.BFGS_Quasi_Newton_Method(function_4_x,grad_function_4_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_4_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([[1,0],[0,1]]),np.array([2,-2]),linear_second_derivative_function_4_alpha,\n",
    "                                                                          1e-6,10)\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.Conjugate_Gradient_Descent_Method(function_4_x,grad_function_4_x,hessian_function_4_x,\n",
    "                                                                          1e-5,1000,np.array([2,-2]))\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def function_5_x(x,in_param,gamma_param,c=1):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return (x[0]-1)**2+(x[1]-1)**2+c*(x[0]**2+x[1]**2-0.25)**2\n",
    "\n",
    "def grad_function_5_x(x,in_param,gamma_param,c=1):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return np.array([(2*(x[0]-1))+(2*c*(x[0]**2+x[1]**2-0.25)*2*x[0]),(2*(x[1]-1))+(2*c*(x[0]**2+x[1]**2-0.25)*2*x[1])])\n",
    "\n",
    "def hessian_function_5_x(x,c=1):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return np.array([[2+2*c*(x[0]**2+x[1]**2-0.25)*2+8*c*x[0]**2,8*c*x[0]*x[1]],[8*c*x[0]*x[1],2+2*c*(x[0]**2+x[1]**2-0.25)*2+8*c*x[1]**2]])\n",
    "\n",
    "\n",
    "def linear_first_derivative_function_5_alpha(alpha,x,p,in_param,gamma_param,c=1):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    p=np.squeeze(np.asarray(p))\n",
    "    nx=x+alpha*p\n",
    "    return 2*nx[0]*p[0]+2*nx[1]*p[1]+(2*c*(nx[0]**2+nx[1]**2-0.25)*(2*nx[0]*p[0]+2*nx[1]*p[1]))\n",
    "\n",
    "def linear_second_derivative_function_5_alpha(alpha,x,p,in_param,gamma_param,c=1):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    p=np.squeeze(np.asarray(p))\n",
    "    nx=x+alpha*p\n",
    "    return 2*p[0]**2+2*p[1]**2+(2*c*(2*nx[0]*p[0]+2*nx[1]*p[1])**2)+(2*c*(nx[0]**2+nx[1]**2-0.25)*2*p[0]**2+2*p[1]**2)\n",
    "\n",
    "optimize=unconstrained_optimizer()\n",
    "[xval,graval,alval,crit]=optimize.Steepest_Gradient_Descent_Method(function_5_x,grad_function_5_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_5_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([1,-1]),linear_second_derivative_function_5_alpha,\n",
    "                                                                          1e-4,0.5)\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crit)\n",
    "print('last value of x:')\n",
    "print(xval[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.Newtons_Method(grad_function_5_x,hessian_function_5_x,1e-5,1000,np.array([1,-1]))\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.BFGS_Quasi_Newton_Method(function_5_x,grad_function_5_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_5_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([[1,0],[0,1]]),np.array([1,-1]),linear_second_derivative_function_5_alpha,\n",
    "                                                                          0.5,0.5)\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.Conjugate_Gradient_Descent_Method(function_5_x,grad_function_5_x,hessian_function_5_x,\n",
    "                                                                          1e-5,1000,np.array([1,-1]))\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application (Optional)\n",
    "Refer to the Report PDF for more details.\n",
    "### Expectation-Maximization Algorithm \n",
    "[EM Algorithm](http://www.rmki.kfki.hu/~banmi/elte/bishop_em.pdf)  (Pages 438 and 439 has been used for reference) is part of MLE used when the marginal probabilietes of data are hard to be optimized\n",
    "\n",
    "##### Assumption Cancerous and Non Cancerous pixels are independent i.e $\\rho$ is zero.\n",
    "\n",
    "\n",
    "step1:  Initialize parameters with seed value.\n",
    "\n",
    "step2:  Find Expection $P(Z=z/x;\\theta)$.\n",
    "\n",
    "step3: Maximize Q with respect to parameters or Minimize Q w.r.t parameters\n",
    "\n",
    "step4: check for convergence of parameters of log likelihood function if yes exit else continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_pdf(inp,mean,variance):\n",
    "    return (1/(2*np.pi*np.sqrt(variance)))*np.exp(-1*((inp-mean)**2/(2*variance)))\n",
    "\n",
    "def marginal_log_likelihood_funtion_x(x,theta):\n",
    "    theta=np.squeeze(np.asarray(theta))\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(theta)!=5:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 5\")\n",
    "    else:\n",
    "        mu1=theta[0]\n",
    "        mu2=theta[2]\n",
    "        v1=theta[1]\n",
    "        v2=theta[3]\n",
    "        P1=theta[4]\n",
    "        P2=1-theta[4]\n",
    "        gauss1=gaussian_pdf(x,mu1,v1)\n",
    "        gauss2=gaussian_pdf(x,mu2,v2)\n",
    "        return np.sum(np.log(P1*gauss1+P2*gauss2))\n",
    "\n",
    "def Expection_function(x,theta):\n",
    "    theta=np.squeeze(np.asarray(theta))\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(theta)!=5:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 5\")\n",
    "    else:\n",
    "        mu1=theta[0]\n",
    "        mu2=theta[2]\n",
    "        v1=theta[1]\n",
    "        v2=theta[3]\n",
    "        P=[theta[4],1-theta[4]]\n",
    "        gauss=[gaussian_pdf(x,mu1,v1),gaussian_pdf(x,mu2,v2)]\n",
    "        denominator=P[0]*gauss[0]+P[1]*gauss[1]\n",
    "        return [P[0]*gauss[0]/denominator,P[1]*gauss[1]/denominator]\n",
    "\n",
    "def Q_function(theta,x,gamma):\n",
    "    theta=np.squeeze(np.asarray(theta))\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(theta)!=5:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 5\")\n",
    "    else:\n",
    "        mu1=theta[0]\n",
    "        mu2=theta[2]\n",
    "        v1=theta[1]\n",
    "        v2=theta[3]\n",
    "        P1=theta[4]\n",
    "        P2=1-theta[4]\n",
    "        gauss1=gaussian_pdf(x,mu1,v1)\n",
    "        gauss2=gaussian_pdf(x,mu2,v2)\n",
    "        Q=np.log(P1*gauss1)*gamma[0]+np.log(P2*gauss2)*gamma[1]\n",
    "        Q=-1*np.sum(Q)\n",
    "        return Q\n",
    "        \n",
    "def grad_Q_function(theta,x,gamma):\n",
    "    theta=np.squeeze(np.asarray(theta))\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(theta)!=5:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 5\")\n",
    "    else:\n",
    "        mu1=theta[0]\n",
    "        mu2=theta[2]\n",
    "        v1=theta[1]\n",
    "        v2=theta[3]\n",
    "        P1=theta[4]\n",
    "        P2=1-theta[4]\n",
    "        dq_dmu1=-1*np.sum(((x-mu1)/v1)*gamma[0])\n",
    "        dq_dv1=-1*np.sum(((-0.5/v1)+(0.5*(x-mu1)**2/v1**2))*gamma[0])\n",
    "        dq_dmu2=-1*np.sum(((x-mu2)/v2)*gamma[1])\n",
    "        dq_dv2=-1*np.sum(((-0.5/v2)+(0.5*(x-mu2)**2/v2**2))*gamma[1])\n",
    "        dq_dp=-1*np.sum((1/P1)*gamma[0]-(1/P2)*gamma[1])\n",
    "        return np.array([dq_dmu1,dq_dv1,dq_dmu2,dq_dv2,dq_dp])\n",
    "\n",
    "\n",
    "def linearfd_Q_func_alpha(alpha,theta,theta_p,x,gamma_param):\n",
    "    theta=np.squeeze(np.asarray(theta))\n",
    "    theta_p=np.squeeze(np.asarray(theta_p))\n",
    "    nt=theta+alpha*theta_p\n",
    "    if len(theta)!=5:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 5\")\n",
    "    else:\n",
    "        mu1=nt[0]\n",
    "        mu2=nt[2]\n",
    "        v1=nt[1]\n",
    "        v2=nt[3]\n",
    "        P1=nt[4]\n",
    "        P2=1-P1\n",
    "        rval=((theta_p[4]/P1)+(0.5*(theta_p[1]/v1))+(0.5*(v1*2*(x-mu1)*-1*theta_p[0]-(x-mu1)**2*theta_p[1])/v1**2))*gamma_param[0]\n",
    "        rval+=((-1*theta_p[4]/P2)+(0.5*(theta_p[3]/v2))+(0.5*(v2*2*(x-mu2)*-1*theta_p[2]-(x-mu2)**2*theta_p[3])/v2**2))*gamma_param[1]\n",
    "        rval=-1*np.sum(rval)\n",
    "        return rval\n",
    "        \n",
    "def linearsd_Q_func_alpha(alpha,theta,theta_p,x,gamma_param):\n",
    "    theta=np.squeeze(np.asarray(theta))\n",
    "    theta_p=np.squeeze(np.asarray(theta_p))\n",
    "    nt=theta+alpha*theta_p\n",
    "    if len(theta)!=5:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 5\")\n",
    "    else:\n",
    "        mu1=nt[0]\n",
    "        mu2=nt[2]\n",
    "        v1=nt[1]\n",
    "        v2=nt[3]\n",
    "        P1=nt[4]\n",
    "        P2=1-P1\n",
    "        n1=v1*2*(x-mu1)*-1*theta_p[0]-(x-mu1)**2*theta_p[1]\n",
    "        n2=v2*2*(x-mu2)*-1*theta_p[2]-(x-mu2)**2*theta_p[3]\n",
    "        dn1=(v1*2*theta_p[0]**2)\n",
    "        dn2=(v2*2*theta_p[2]**2)\n",
    "        rval=((-1*theta_p[4]**2/P1**2)+(0.5*(-1*theta_p[1]**2/v1**2))+(-1*(v1**2*dn1-n1*2*v1*theta_p[1])/v1**3))*gamma_param[0]\n",
    "        rval+=((theta_p[4]**2/P2**2)+(0.5*(-1*theta_p[3]**2/v2**2))+(-1*(v2**2*dn2-n2*2*v2*theta_p[3])/v2**3))*gamma_param[1]\n",
    "        rval=-1*np.sum(rval)\n",
    "        return rval\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_M_Algorithm(file_name,seed_theta):\n",
    "    x=np.loadtxt(file_name,dtype=int,delimiter='\\n')\n",
    "    optimize=unconstrained_optimizer()\n",
    "    seed_theta=np.array(seed_theta)\n",
    "    inital_p_x_given_theta=marginal_log_likelihood_funtion_x(x,seed_theta)\n",
    "    \n",
    "    if len(seed_theta)!=5:\n",
    "        sys.exit(\"E_M can handle only vector of size 5\")\n",
    "   \n",
    "    for i in range(100):\n",
    "        # Expectation step\n",
    "        gamma=Expection_function(x,seed_theta)\n",
    "        \n",
    "        # Maximization Step/ minimization step\n",
    "            \n",
    "            \n",
    "        [new_theta,graval,alval,crit]=optimize.Steepest_Gradient_Descent_Method(Q_function,grad_Q_function,\n",
    "                                                                          Newton_line_search,linearfd_Q_func_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,seed_theta,linearsd_Q_func_alpha,\n",
    "                                                                          1e-4,1e-3,x,gamma)\n",
    "        new_p_x_given_theta=marginal_log_likelihood_funtion_x(x,new_theta[-1])\n",
    "        seed_theta=new_theta[-1]\n",
    "        c=np.sqrt((new_p_x_given_theta-inital_p_x_given_theta)**2)\n",
    "        if(c<=1e-5):\n",
    "            print(\"Convergence of Marginal probability has happened\")\n",
    "            return seed_theta\n",
    "        else:\n",
    "            inital_p_x_given_theta=new_p_x_given_theta\n",
    "    \n",
    "    print(\"Convergence of Marginal probability has not happened\")\n",
    "    return seed_theta\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "theta=E_M_Algorithm(\"image_samples.txt\",[120.3091,185.1229,201.7558,201.440,0.61739])\n",
    "\n",
    "print(\"Mean1:\")\n",
    "print(theta[0])\n",
    "print(\"Var1:\")\n",
    "print(theta[1])\n",
    "print(\"Mean2:\")\n",
    "print(theta[2])\n",
    "print(\"Var2:\")\n",
    "print(theta[3])\n",
    "print(\"P:\")\n",
    "print(theta[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
