{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECE-505 Computer Project 1\n",
    "### Due Date Nov-6 2018\n",
    "\n",
    "Step 1: Importing necessary libraries for all the necessary modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART1-Algorithmic Implementation\n",
    "#### 1.Steepest Gradient Descent Method\n",
    "Details of problem statement for implementing Steepest Gradient Descent Method is available in the file \"Computer Project I description.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unconstrained_optimizer:\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    \n",
    "    #function is the mathematical function that needs to be optimized\n",
    "    #gradient is the gadient of the mathematical function.\n",
    "    #line_search is method of finding alpha\n",
    "    def Steepest_Gradient_Descent_Method(self,function,gradient,line_search,\n",
    "                                         linear_first_derivative,condition_check,\n",
    "                                         epsilon,Iter_max,seed_x,linear_second_derivative=None,condition_param=0.5,seed_alpha=0.5):\n",
    "        #List to store values to be printed at each iteration\n",
    "        grad_value=[]\n",
    "        x_value=[]\n",
    "        alpha_value=[]\n",
    "        next_x=np.array(seed_x)\n",
    "        current_alpha_value=seed_alpha\n",
    "        for i in range(Iter_max):\n",
    "            #print(\"Iteration:\")\n",
    "            #print(i)\n",
    "            current_x=next_x\n",
    "            current_grad_value=gradient(current_x)\n",
    "            x_value.append(current_x)\n",
    "            grad_value.append(current_grad_value)\n",
    "            criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))\n",
    "            \n",
    "            if criteria <= epsilon:\n",
    "                print(\"Criteria of epsilon met\")\n",
    "                return[x_value,grad_value,alpha_value,criteria]\n",
    "            if i==25 and condition_param<1e-3:\n",
    "                condition_param=0.8\n",
    "                alpha=0.9\n",
    "            elif i==50 and condition_param<1e-3:\n",
    "                condition_param=0.8\n",
    "                alpha=0.9\n",
    "            elif i==100 and condition_param<1e-2:\n",
    "                condition_param=0.8\n",
    "                alpha=0.9\n",
    "            elif i==200 and condition_param<1e-1:\n",
    "                condition_param=0.8\n",
    "                alpha=0.9\n",
    "            elif i==300:\n",
    "                alpha=1.5\n",
    "            elif i==500 :\n",
    "                condition_param=0.8\n",
    "                alpha=1.5\n",
    "            elif i==700:\n",
    "                alpha=1.5\n",
    "            \n",
    "            p=-current_grad_value\n",
    "            current_alpha_value=line_search(current_x,p,current_alpha_value,function,current_grad_value, \n",
    "                                            linear_first_derivative,linear_second_derivative,condition_check,\n",
    "                                            (condition_param))\n",
    "            alpha_value.append(current_alpha_value)\n",
    "            next_x=current_x+(current_alpha_value*p)\n",
    "        \n",
    "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
    "        current_x=next_x\n",
    "        current_grad_value=gradient(current_x)\n",
    "        x_value.append(current_x)\n",
    "        grad_value.append(current_grad_value)\n",
    "        criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))        \n",
    "        return[x_value,grad_value,alpha_value,criteria]\n",
    "    \n",
    "    \n",
    "    def Newtons_Method(self,gradient_function,hessian_function,epsilon,Iter_max,seed_x):\n",
    "        x_value=[]\n",
    "        grad_value=[]\n",
    "        direction_value=[]\n",
    "        next_x=np.array(seed_x)\n",
    "        for i in range(Iter_max):\n",
    "            #print(\"Iteration :\")\n",
    "            #print(i)\n",
    "            current_x_value=next_x\n",
    "            current_grad_value=gradient_function(current_x_value)\n",
    "            criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))\n",
    "            x_value.append(current_x_value)\n",
    "            grad_value.append(current_grad_value)\n",
    "            if criteria <= epsilon:\n",
    "                print(\"Criteria of epsilon met\")\n",
    "                return[x_value,grad_value,direction_value,criteria]\n",
    "            current_hessian_value=hessian_function(current_x_value)\n",
    "            current_direction_value=np.linalg.solve(current_hessian_value,current_grad_value)\n",
    "            direction_value.append(current_direction_value)\n",
    "            current_hessian_inv=np.linalg.inv(current_hessian_value)\n",
    "            next_x=current_x_value-current_hessian_inv.dot(current_direction_value)\n",
    "            \n",
    "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
    "        current_x_value=next_x\n",
    "        current_grad_value=gradient_function(current_x_value)\n",
    "        criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))\n",
    "        x_value.append(current_x_value)\n",
    "        grad_value.append(current_grad_value)\n",
    "        return[x_value,grad_value,direction_value,criteria]\n",
    "    \n",
    "    def BFGS_Quasi_Newton_Method(self,function,gradient_function,line_search,\n",
    "                                 linear_first_derivative,condition_check,\n",
    "                                 epsilon,Iter_max,seed_Hessian,seed_x,\n",
    "                                 linear_second_derivative=None,condition_param=0.5,seed_alpha=0.5):\n",
    "        x_value=[]\n",
    "        grad_value=[]\n",
    "        direction_value=[]\n",
    "        next_x=np.matrix(seed_x).T\n",
    "        next_B=np.matrix(seed_Hessian)\n",
    "        next_gradient=np.matrix(gradient_function(next_x)).T\n",
    "        alpha=seed_alpha\n",
    "        for i in range(Iter_max):\n",
    "            current_x_value=next_x\n",
    "            current_B=next_B\n",
    "            current_grad_value=next_gradient\n",
    "            x_value.append(current_x_value)\n",
    "            grad_value.append(current_grad_value)\n",
    "            criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))\n",
    "            if criteria <= epsilon:\n",
    "                print(\"Criteria of epsilon met\")\n",
    "                return[x_value,grad_value,direction_value,criteria]\n",
    "            current_direction_value=np.linalg.solve(current_B,current_grad_value)\n",
    "            direction_value.append(current_direction_value)\n",
    "            alpha=line_search(current_x_value,current_direction_value,alpha,function,current_grad_value, \n",
    "                                            linear_first_derivative,linear_second_derivative,condition_check,\n",
    "                                            condition_param)\n",
    "            next_x=current_x_value+(alpha*current_direction_value)\n",
    "            next_gradient=np.matrix(gradient_function(next_x)).T\n",
    "            s_k=next_x-current_x_value\n",
    "            y_k=next_gradient-current_grad_value\n",
    "            next_B=current_B-((((current_B*s_k)*(current_B*s_k).T)/(s_k.T*current_B*s_k)))+((y_k*y_k.T)/(y_k.T*s_k))\n",
    "        \n",
    "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
    "        current_x_value=next_x\n",
    "        current_grad_value=gradient_function(current_x_value)\n",
    "        criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))\n",
    "        x_value.append(current_x_value)\n",
    "        grad_value.append(current_grad_value)\n",
    "        return[x_value,grad_value,direction_value,criteria]\n",
    "    \n",
    "    def Conjugate_Gradient_Descent_Method(self,line_search,gradient_function,epsilon,Iter_max,seed_x):\n",
    "        x_value=[]\n",
    "        grad_value=[]\n",
    "        direction_value=[]\n",
    "        next_x=np.matrix(seed_x).T\n",
    "        next_gradient=np.matrix(gradient_function(next_x)).T\n",
    "        next_direction=-1*next_gradient\n",
    "        \n",
    "        for i in range(Iter_max):\n",
    "            current_x=next_x\n",
    "            current_gradient=next_gradient\n",
    "            current_direction=next_direction\n",
    "            x_value.append(current_x)\n",
    "            grad_value.append(current_gradient)\n",
    "            criteria=np.linalg.norm(current_gradient,2)/(1+np.linalg.norm(current_gradient,1))\n",
    "            \n",
    "            if criteria <= epsilon:\n",
    "                print(\"Criteria of epsilon met\")\n",
    "                return[x_value,grad_value,direction_value,criteria]\n",
    "            alpha=line_search(current_x)\n",
    "            next_x=current_x+alpha*current_direction\n",
    "            next_gradient=np.matrix(gradient_function(next_x)).T\n",
    "            beta=np.linalg.norm(current_gradient,2)**2/np.linalg.norm(next_gradient,2)\n",
    "            next_direction=-1*next_gradient+(beta*current_direction)\n",
    "        \n",
    "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
    "        current_x=next_x\n",
    "        x_value.append(current_x)\n",
    "        return[x_value,grad_value,direction_value,criteria]\n",
    "    \n",
    "\n",
    "def Newton_line_search(x,p,alpha,function,grad_val,linear_first_derivative,linear_second_derivative,condition_check,condition_param):\n",
    "    next_alpha=alpha\n",
    "    condition=condition_check(condition_param,function,grad_val,x,p,next_alpha)\n",
    "    while(condition==False):\n",
    "        current_alpha=next_alpha\n",
    "        first_deri=linear_first_derivative(current_alpha,x,p)\n",
    "        second_deri=linear_second_derivative(current_alpha,x,p)\n",
    "        next_alpha=current_alpha-(first_deri/second_deri)\n",
    "        condition=condition_check(condition_param,function,grad_val,x,p,next_alpha)\n",
    "        \n",
    "    return next_alpha\n",
    "\n",
    "#def secant_line_search(x,p,alpha,function,linear_first_derivative,linear_second_derivative,condition_check,condition_param):\n",
    "#    condition=False\n",
    "#    next_alpha=alpha\n",
    "#    while(condition==False):\n",
    "#        current_alpha=next_alpha\n",
    "#        first_deri=linear_first_derivative(current_alpha,x,p)\n",
    "#        second_deri=linear_second_derivative(current_alpha,x,p)\n",
    "#        next_alpha=current_alpha-(first_deri/second_deri)\n",
    "#        condition=condition_check(condition_param,function,x,p,next_alpha)\n",
    "#        \n",
    "#    return next_alpha   \n",
    "\n",
    "#def Wolfe_Condition(condition_param,function,x,p,next_alpha):\n",
    "#    pass\n",
    "\n",
    "def Armijo_Condition(condition_param,function,grad_val,x,p,alpha):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    p=np.squeeze(np.asarray(p))\n",
    "    grad_val=np.squeeze(np.asarray(grad_val))\n",
    "    LHS=function(x+alpha*p)\n",
    "    RHS=function(x)+(condition_param*alpha*np.dot(p,grad_val))\n",
    "    if LHS<=RHS:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criteria of epsilon met\n",
      "Values of crit :\n",
      "0.0\n",
      "last value of x:\n",
      "[0. 0. 0.]\n",
      "Criteria of epsilon met\n",
      "Values of crit :\n",
      "6.607173866452323e-06\n",
      "last value of x:\n",
      "[1.90734863e-06 1.90734863e-06 1.90734863e-06]\n",
      "Criteria of epsilon met\n"
     ]
    }
   ],
   "source": [
    "def function_1_x(x):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=3:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 3\")\n",
    "    else:\n",
    "        return x[0]**2+x[1]**2+x[2]**2\n",
    "\n",
    "def grad_function_1_x(x):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=3:\n",
    "        print(len(x))\n",
    "        sys.exit(\"fuction1 can handle only vector of size 3\")\n",
    "    else:\n",
    "        return np.array([2*x[0],2*x[1],2*x[2]])\n",
    "    \n",
    "def hessian_function_1_x(x):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    if len(x)!=3:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 3\")\n",
    "    else:\n",
    "        return np.array([[2,0,0],[0,2,0],[0,0,2]])\n",
    "\n",
    "def linear_first_derivative_function_1_alpha(alpha,x,p):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    p=np.squeeze(np.asarray(p))\n",
    "    return 2*np.dot(x+alpha*p,p) \n",
    "\n",
    "def linear_second_derivative_function_1_alpha(alpha,x,p):\n",
    "    x=np.squeeze(np.asarray(x))\n",
    "    p=np.squeeze(np.asarray(p))\n",
    "    return 2*np.dot(p,p)\n",
    "\n",
    "optimize=unconstrained_optimizer()\n",
    "[xval,graval,alval,crit]=optimize.Steepest_Gradient_Descent_Method(function_1_x,grad_function_1_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_1_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([1,1,1]),linear_second_derivative_function_1_alpha,\n",
    "                                                                          0.5,0.5)\n",
    "#print('Values of x :')\n",
    "#print(xval)\n",
    "#print('Values of func :')\n",
    "#print(funval)\n",
    "#print('Values of grad :')\n",
    "#print(graval)\n",
    "#print('Values of alpha :')\n",
    "#print(alval)\n",
    "print('Values of crit :')\n",
    "print(crit)\n",
    "print('last value of x:')\n",
    "print(xval[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.Newtons_Method(grad_function_1_x,hessian_function_1_x,1e-5,1000,np.array([1,1,1]))\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.BFGS_Quasi_Newton_Method(function_1_x,grad_function_1_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_1_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([[1,0,0],[0,1,0],[0,0,1]]),np.array([1,1,1]),linear_second_derivative_function_1_alpha,\n",
    "                                                                          0.5,0.5)\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2_x(x):\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return x[0]**2+2*x[1]**2-2*x[0]*x[1]-2*x[1]\n",
    "\n",
    "def grad_function_2_x(x):\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return np.array([2*x[0]-2*x[1],4*x[1]-2*x[0]-2])\n",
    "\n",
    "def hessian_function_2_x(x):\n",
    "    return np.array([[2,-2],[-2,4]])\n",
    "\n",
    "def linear_first_derivative_function_2_alpha(alpha,x,p):\n",
    "    nx=x+alpha*p\n",
    "    return 2*nx[0]*p[0]+4*nx[1]*p[1]-2*p[0]*nx[1]-2*p[1]*nx[0]-2*p[1]\n",
    "\n",
    "def linear_second_derivative_function_2_alpha(alpha,x,p):\n",
    "    return 2*p[0]**2+4*p[1]**2-4*p[1]*p[0]\n",
    "\n",
    "optimize=unconstrained_optimizer()\n",
    "[xval,graval,alval,crit]=optimize.Steepest_Gradient_Descent_Method(function_2_x,grad_function_2_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_2_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([0,0]),linear_second_derivative_function_2_alpha,\n",
    "                                                                          0.01,1)\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crit)\n",
    "print('last value of x:')\n",
    "print(xval[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.Newtons_Method(grad_function_2_x,hessian_function_2_x,1e-5,1000,np.array([0,0]))\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_3_x(x):\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return (100*(x[1]-x[0]**2)**2)+((1-x[0])**2)\n",
    "\n",
    "def grad_function_3_x(x):\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return np.array([200*(x[1]-x[0]**2)*(-2*x[0])+(-2*(1-x[0])),200*(x[1]-x[0]**2)])\n",
    "\n",
    "def hessian_function_3_x(x):\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return np.array([[(1200*x[0]**2)-(400*x[1])+2,-400*x[0]],[-400*x[0],200]])\n",
    "\n",
    "def linear_first_derivative_function_3_alpha(alpha,x,p):\n",
    "    nx=x+alpha*p\n",
    "    return 200*(nx[1]-nx[0]**2)*(p[1]-2*nx[0]*p[0])-2*(1-nx[0])*p[0]\n",
    "\n",
    "def linear_second_derivative_function_3_alpha(alpha,x,p):\n",
    "    nx=x+alpha*p\n",
    "    return 200*(p[1]-2*nx[0]*p[0])*(-2*p[0]**2)+2*p[0]**2\n",
    "\n",
    "optimize=unconstrained_optimizer()\n",
    "[xval,graval,alval,crit]=optimize.Steepest_Gradient_Descent_Method(function_3_x,grad_function_3_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_3_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([-1.2,1]),linear_second_derivative_function_3_alpha,\n",
    "                                                                          1e-4,1e-3)\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crit)\n",
    "print('last value of x:')\n",
    "print(xval[-1])\n",
    "\n",
    "[x_val,grad_val,dir_val,crite]=optimize.Newtons_Method(grad_function_3_x,hessian_function_3_x,1e-5,1000,np.array([-1.2,1]))\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crite)\n",
    "print('last value of x:')\n",
    "print(x_val[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_4_x(x):\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return (x[0]+x[1])**4 + x[1]**2\n",
    "\n",
    "def grad_function_4_x(x):\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return np.array([4*(x[0]+x[1])**3,4*(x[0]+x[1])**3+ 2*x[1]])\n",
    "\n",
    "def linear_first_derivative_function_4_alpha(alpha,x,p):\n",
    "    nx=x+alpha*p\n",
    "    return (4*((nx[0]+nx[1])**3)*(p[0]+p[1]))+2*nx[1]*p[1]\n",
    "\n",
    "def linear_second_derivative_function_4_alpha(alpha,x,p):\n",
    "    nx=x+alpha*p\n",
    "    return (12*((nx[0]+nx[1])**2)*(p[0]+p[1])**2)+2*p[1]**2\n",
    "\n",
    "optimize=unconstrained_optimizer()\n",
    "[xval,graval,alval,crit]=optimize.Steepest_Gradient_Descent_Method(function_4_x,grad_function_4_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_4_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([2,-2]),linear_second_derivative_function_4_alpha,\n",
    "                                                                          0.5,0.5)\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crit)\n",
    "print('last value of x:')\n",
    "print(xval[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_5_x(x,c=1):\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return (x[0]-1)**2+(x[1]-1)**2+c*(x[0]**2+x[1]**2-0.25)**2\n",
    "\n",
    "def grad_function_5_x(x,c=1):\n",
    "    if len(x)!=2:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 2\")\n",
    "    else:\n",
    "        return np.array([(2*(x[0]-1))+(2*c*(x[0]**2+x[1]**2-0.25)*2*x[0]),(2*(x[1]-1))+(2*c*(x[0]**2+x[1]**2-0.25)*2*x[1])])\n",
    "\n",
    "def linear_first_derivative_function_5_alpha(alpha,x,p,c=1):\n",
    "    nx=x+alpha*p\n",
    "    return 2*nx[0]*p[0]+2*nx[1]*p[1]+(2*c*(nx[0]**2+nx[1]**2-0.25)*(2*nx[0]*p[0]+2*nx[1]*p[1]))\n",
    "\n",
    "def linear_second_derivative_function_5_alpha(alpha,x,p,c=1):\n",
    "    nx=x+alpha*p\n",
    "    return 2*p[0]**2+2*p[1]**2+(2*c*(2*nx[0]*p[0]+2*nx[1]*p[1])**2)+(2*c*(nx[0]**2+nx[1]**2-0.25)*2*p[0]**2+2*p[1]**2)\n",
    "\n",
    "optimize=unconstrained_optimizer()\n",
    "[xval,graval,alval,crit]=optimize.Steepest_Gradient_Descent_Method(function_5_x,grad_function_5_x,\n",
    "                                                                          Newton_line_search,linear_first_derivative_function_5_alpha,\n",
    "                                                                          Armijo_Condition,\n",
    "                                                                          1e-5,1000,np.array([1,-1]),linear_second_derivative_function_5_alpha,\n",
    "                                                                          0.5,0.5)\n",
    "\n",
    "print('Values of crit :')\n",
    "print(crit)\n",
    "print('last value of x:')\n",
    "print(xval[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
