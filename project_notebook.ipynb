{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECE-505 Computer Project 1\n",
    "### Due Date Nov-6 2018\n",
    "\n",
    "Step 1: Importing necessary libraries for all the necessary modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART1-Algorithmic Implementation\n",
    "#### 1.Steepest Gradient Descent Method\n",
    "Details of problem statement for implementing Steepest Gradient Descent Method is available in the file \"Computer Project I description.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unconstrained_optimizer:\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    \n",
    "    #function is the mathematical function that needs to be optimized\n",
    "    #gradient is the gadient of the mathematical function.\n",
    "    #line_search is method of finding alpha\n",
    "    def Steepest_Gradient_Descent_Method(self,function,gradient,line_search,epsilon,Iter_max,seed_x):\n",
    "        #List to store values to be printed at each iteration\n",
    "        func_value=[]\n",
    "        grad_value=[]\n",
    "        x_value=[]\n",
    "        alpha_value=[]\n",
    "        next_x=np.array(seed_x)\n",
    "        for i in range(Iter_max):\n",
    "            current_x=next_x\n",
    "            current_func_value=function(current_x)\n",
    "            current_grad_value=gradient(current_x)\n",
    "            x_value.append(current_x)\n",
    "            func_value.append(current_func_value)\n",
    "            grad_value.append(current_grad_value)\n",
    "            criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))\n",
    "            \n",
    "            if criteria <= epsilon:\n",
    "                print(\"Criteria of epsilon met\")\n",
    "                return[x_value,func_value,grad_value,alpha_value,criteria]\n",
    "            \n",
    "            current_alpha_value=line_search(current_x)\n",
    "            alpha_value.append(current_alpha_value)\n",
    "            p=-current_grad_value\n",
    "            next_x=current_x+(current_alpha_value*p)\n",
    "        \n",
    "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
    "        current_x=next_x\n",
    "        current_func_value=function(current_x)\n",
    "        current_grad_value=gradient(current_x)\n",
    "        x_value.append(current_x)\n",
    "        func_value.append(current_func_value)\n",
    "        grad_value.append(current_grad_value)\n",
    "        criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))        \n",
    "        return[x_value,func_value,grad_value,alpha_value,criteria]\n",
    "    \n",
    "    \n",
    "    def Newtons_Method(self,gradient_function,hessian_function,epsilon,Iter_max,seed_x):\n",
    "        x_value=[]\n",
    "        grad_value=[]\n",
    "        direction_value=[]\n",
    "        next_x=np.array(seed_x)\n",
    "        for i in range(Iter_max):\n",
    "            current_x_value=next_x\n",
    "            current_grad_value=gradient_function(current_x_value)\n",
    "            criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))\n",
    "            x_value.append(current_x)\n",
    "            grad_value.append(current_grad_value)\n",
    "            if criteria <= epsilon:\n",
    "                print(\"Criteria of epsilon met\")\n",
    "                return[x_value,grad_value,direction_value,criteria]\n",
    "            current_hessian_value=hessian_function(current_x_value)\n",
    "            current_direction_value=np.linalg.solve(current_hessian_value,current_gradient)\n",
    "            direction_value.append(current_direction_value)\n",
    "            next_x=current_x_value-current_hessian_value.dot(current_direction_value)\n",
    "        \n",
    "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
    "        current_x=next_x\n",
    "        x_value.append(current_x)\n",
    "        return[x_value,grad_value,direction_value,criteria]\n",
    "    \n",
    "    def BFGS_Quasi_Newton_Method(self,gradient_function,line_search,epsilon,Iter_max,seed_Hessian,seed_x):\n",
    "        x_value=[]\n",
    "        grad_value=[]\n",
    "        direction_value=[]\n",
    "        next_x=np.matrix(seed_x).T\n",
    "        next_B=np.matrix(seed_Hessian)\n",
    "        next_gradient=gradient_function(next_x)\n",
    "        for i in range(Iter_max):\n",
    "            current_x_value=next_x\n",
    "            current_B=next_B\n",
    "            current_grad_value=next_gradient\n",
    "            x_value.append(current_x)\n",
    "            grad_value.append(current_grad_value)\n",
    "            criteria=np.linalg.norm(current_grad_value,2)/(1+np.linalg.norm(current_grad_value,1))\n",
    "            if criteria <= epsilon:\n",
    "                print(\"Criteria of epsilon met\")\n",
    "                return[x_value,grad_value,direction_value,criteria]\n",
    "            current_direction_value=np.linalg.solve(current_B,current_gradient)\n",
    "            direction_value.append(current_direction_value)\n",
    "            alpha=line_search(current_x)\n",
    "            next_x=current_x_value+(alpha*current_direction_value)\n",
    "            next_gradient=gradient_function(next_x)\n",
    "            s_k=next_x-current_x\n",
    "            y_k=next_gradient-current_grad_value\n",
    "            next_B=current_B-((((current_B*s_k)*(current_B*s_k).T)/(s_k.T*current_B*s_k)))+((y_k*y_k.T)/(y_k.T*s_k))\n",
    "        \n",
    "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
    "        current_x=next_x\n",
    "        x_value.append(current_x)\n",
    "        return[x_value,grad_value,direction_value,criteria]\n",
    "    \n",
    "    def Conjugate_Gradient_Descent_Method(self,line_search,gradient_function,epsilon,Iter_max,seed_x):\n",
    "        x_value=[]\n",
    "        grad_value=[]\n",
    "        direction_value=[]\n",
    "        next_x=np.matrix(seed_x).T\n",
    "        next_gradient=np.matrix(gradient_function(next_x)).T\n",
    "        next_direction=-1*next_gradient\n",
    "        \n",
    "        for i in range(Iter_max):\n",
    "            current_x=next_x\n",
    "            current_gradient=next_gradient\n",
    "            current_direction=next_direction\n",
    "            x_value.append(current_x)\n",
    "            grad_value.append(current_gradient)\n",
    "            criteria=np.linalg.norm(current_gradient,2)/(1+np.linalg.norm(current_gradient,1))\n",
    "            \n",
    "            if criteria <= epsilon:\n",
    "                print(\"Criteria of epsilon met\")\n",
    "                return[x_value,grad_value,direction_value,criteria]\n",
    "            alpha=line_search(current_x)\n",
    "            next_x=current_x+alpha*current_direction\n",
    "            next_gradient=np.matrix(gradient_function(next_x)).T\n",
    "            beta=np.linalg.norm(current_gradient,2)**2/np.linalg.norm(next_gradient,2)\n",
    "            next_direction=-1*next_gradient+(beta*current_direction)\n",
    "        \n",
    "        print(\"Maximum iteration reached.Criteria of epsilon not met\")\n",
    "        current_x=next_x\n",
    "        x_value.append(current_x)\n",
    "        return[x_value,grad_value,direction_value,criteria]\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criteria of epsilon met\n",
      "Values of crit :\n",
      "8.294756262879721e-06\n",
      "last value of x:\n",
      "[2.39452428e-06 2.39452428e-06 2.39452428e-06]\n"
     ]
    }
   ],
   "source": [
    "def function_1_x(x):\n",
    "    if len(x)!=3:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 3\")\n",
    "    else:\n",
    "        return x[0]**2+x[1]**2+x[2]**2\n",
    "\n",
    "def grad_function_1_x(x):\n",
    "    if len(x)!=3:\n",
    "        sys.exit(\"fuction1 can handle only vector of size 3\")\n",
    "    else:\n",
    "        return np.array([2*x[0],2*x[1],2*x[2]])\n",
    "\n",
    "def func_1_linear_search_exact(alpha_k,pk,xk):\n",
    "    return alpha_k-df/df_2\n",
    "\n",
    "def dummy_line_search(x):\n",
    "    return 0.1\n",
    "\n",
    "optimize=unconstrained_optimizer()\n",
    "[xval,funval,graval,alval,crit]=optimize.Steepest_Gradient_Descent_Method(function_1_x,grad_function_1_x,dummy_line_search,1e-5,1000,np.array([1,1,1]))\n",
    "#print('Values of x :')\n",
    "#print(xval)\n",
    "#print('Values of func :')\n",
    "#print(funval)\n",
    "#print('Values of grad :')\n",
    "#print(graval)\n",
    "#print('Values of alpha :')\n",
    "#print(alval)\n",
    "print('Values of crit :')\n",
    "print(crit)\n",
    "print('last value of x:')\n",
    "print(xval[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
